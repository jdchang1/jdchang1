<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jonathan D. Chang</title>
  
  <meta name="author" content="Jonathan D. Chang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/cornell_logo.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jonathan D. Chang</name>
              </p>
              <p>I am a 5th year Ph.D. student in the <a href="https://www.cs.cornell.edu">Computer Science department</a> at Cornell University, advised by <a href="https://wensun.github.io">Wen Sun</a>.
              Before Cornell, I recieved my bachelors in Computer Science and Applied Mathematics from Brown University in 2018, working with Stefanie Tellex and George Konidaris 
              in the Humans to Robots (H2R) Laboratory.
              </p>
              <p style="text-align:center">
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="mailto:jdc396@cornell.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=_qY_t5kAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/jdchang1/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jchang1730/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in machine learning, specifically imitation learning and reinforcement learning, and its intersection with generative models. I am particularly interested in studying how to leverage expert demonstrations and learned feature representations for scalable, efficient reinforcement learning. Recently, I have been invested in investigating imitation learning and reinforcement learning for Large Language Models, improving sample efficiency, reducing reward hacking, and developing improved RLHF algorithms.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- <a href="images/milo_front_page.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/milo_front_page.png" class="hoverZoomLink"></a> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://github.com/jdchang1/milo"> -->
              <!--   <papertitle>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage</papertitle> -->
              <!-- </a> -->
              <a>
                <papertitle>Provably Efficient RL with Preference-based Feedback via Dataset Reset</papertitle>
              </a>
              <br>
                            <strong>Jonathan D. Chang*</strong>,
							<a href="https://whzhan99.github.io">Wenhao Zhan*</a>,
							<a href="https://xkianteb.github.io">Kianté Brantley</a>,
							<a href="https://dipendramisra.com">Dipendra Misra</a>,
							<a href="https://jasondlee88.github.io/index.html">Jason D. Lee</a>
							<a href="https://wensun.github.io">Wen Sun</a>
              <br>
                <em>Preprint</em>, 2024
              <br>
              arXiv upcoming
              <p></p>
              <p>New RLHF algorithm with guarantees leveraging the idea of dataset resets, to strictly improve upon PPO with no additional computation costs.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/tril.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/tril.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://github.com/jdchang1/milo"> -->
              <!--   <papertitle>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage</papertitle> -->
              <!-- </a> -->
              <a>
                <papertitle>TRIL: Transformers Reinforcement and Imitation Learning Algorithm</papertitle>
              </a>
              <br>
              <strong>Jonathan D. Chang*</strong>,
							<a href="https://xkianteb.github.io">Kianté Brantley*</a>,
							<a href="https://www.rajkumarramamurthy.com">Rajkumar Ramamurthy</a>,
							<a href="https://dipendramisra.com">Dipendra Misra</a>,
							<a href="https://wensun.github.io">Wen Sun</a>
              <br>
                <em>Repository</em>, 2024
              <br>
              <a href="https://github.com/Cornell-RL/tril">Github</a>
              <p></p>
              <p>Developed, distributed research codebase for RL and IL algorithm development with large LLMs. Deepspeed, transformers, PEFT, and FSDP integration.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/ranking.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ranking.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://github.com/jdchang1/milo"> -->
              <!--   <papertitle>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage</papertitle> -->
              <!-- </a> -->
              <a>
                <papertitle>Policy-Gradient Training of Large Language Models for Ranking</papertitle>
              </a>
              <br>
              <a href="https://gao-g.github.io">Ge Gao</a>,
              <strong>Jonathan D. Chang*</strong>,
							<a href="https://www.cs.cornell.edu/home/cardie/">Claire Cardie</a>,
							<a href="https://xkianteb.github.io">Kianté Brantley*</a>,
							<a href="https://www.cs.cornell.edu/people/tj/">Thorsten Joachims</a>
              <br>
                <em>FMDM@NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2310.04407.pdf">arXiv</a>
              <p></p>
              <p>RL finetuning of retrieval models with a Plackett-Luce ranking policy.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/rlgf.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/rlgf.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://github.com/jdchang1/milo"> -->
              <!--   <papertitle>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage</papertitle> -->
              <!-- </a> -->
              <a>
                <papertitle>Learning to Generate Better than Your LLM</papertitle>
              </a>
              <br>
                            <strong>Jonathan D. Chang*</strong>,
							<a href="https://xkianteb.github.io">Kianté Brantley*</a>,
							<a href="https://www.rajkumarramamurthy.com">Rajkumar Ramamurthy</a>,
							<a href="https://dipendramisra.com">Dipendra Misra</a>,
							<a href="https://wensun.github.io">Wen Sun</a>
              <br>
                <em>Preprint</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2306.11816.pdf">arXiv</a>
              <p></p>
              <p>RLHF algorithmic framework combining two models, our LLM and a suboptimal expert LLM, to improve RLHF performance.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/boost.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/boost.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://github.com/jdchang1/milo"> -->
              <!--   <papertitle>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage</papertitle> -->
              <!-- </a> -->
              <a>
                <papertitle>Adversarial Imitation Learning via Boosting</papertitle>
              </a>
              <br>
                            <strong>Jonathan D. Chang</strong>,
							Dhruv Sreenivas*, 
							<a href="linkedin.com/in/yingbing-huang-3862461aa">Yingbing Huang*</a>, 
							<a href="https://xkianteb.github.io">Kianté Brantley</a>,
							<a href="https://wensun.github.io">Wen Sun</a>
              <br>
                <em>ICLR 2024</em>, 2024 
              <br>
              <a href="https://openreview.net/forum?id=DuQkqSe9en">OpenReview</a>
              <p></p>
              <p>Principled off-policy imitatation learning algorithms through functional gradient boosting. Improves upon SOTA off-policy IL algorithms such as IQLearn.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/bcrl.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/bcrl.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://github.com/jdchang1/milo"> -->
              <!--   <papertitle>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage</papertitle> -->
              <!-- </a> -->
              <a>
                <papertitle>Learning Bellman Complete Representations for Offline Policy Evaluation</papertitle>
              </a>
              <br>
              <strong>Jonathan D. Chang*</strong>,
							<a href="https://kaiwenw.github.io/">Kaiwen Wang*</a>, 
							<a href="https://nathankallus.com/">Nathan Kallus</a>,
							<a href="https://wensun.github.io">Wen Sun</a>
              <br>
                <em>ICML 2022 <font color="#B22222">(Long Talk)</font></em>, 2022 
              <br>
              <a href="https://github.com/CausalML/bcrl">code</a>
              /
              <a href="https://arxiv.org/abs/2207.05837">arXiv</a>
              <p></p>
              <p>Representation learning for Offline Policy Evaluation (OPE) guided by Bellman Completeness and coverage. BCRL achieves state-of-the-art evaluation on image based, continuous control tasks from Deepmind Control Suite.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/milo_front_page.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/milo_front_page.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/jdchang1/milo">
                <papertitle>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage</papertitle>
              </a>
              <br>
                            <strong>Jonathan D. Chang*</strong>,
							<a href="https://masatoshiuehara.com">Masatoshi Uehara*</a>, 
							Dhruv Sreenivas,
							<a href="http://rahulkidambi.github.io">Rahul Kidambi</a>,
							<a href="https://wensun.github.io">Wen Sun</a>
              <br>
              <em>NeurIPS 2021</em>, 2021 
              <br>
              <a href="https://github.com/jdchang1/milo">code</a>
              /
              <a href="https://arxiv.org/abs/2106.03207">arXiv</a>
              <p></p>
              <p>Leveraging offline data with only partial coverage, MILO mitigates covariate shift in imitation learning.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/mobile_poster.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mobile_poster.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.10769">
                <papertitle>MobILE: Model-Based Imitation Learning from Observation Alone</papertitle>
              </a>
              <br>
							<a href="http://rahulkidambi.github.io">Rahul Kidambi</a>,
                            <strong>Jonathan D. Chang</strong>,
							<a href="https://wensun.github.io">Wen Sun</a>
              <br>
              <em>NeurIPS 2021</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2102.10769">arXiv</a>
              <p></p>
              <p>We show that model-based imitation learning from observations (IfO) with strategic exploration can near-optimally solve IfO both in theory and in practice.</p>
            </td>
          </tr> 
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/overview_figure.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/overview_figure.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.10628">
                <papertitle>Learning Deep Parameterized Skills from Demonstration for Re-targetable Visuomotor Control</papertitle>
              </a>
              <br>
                            <strong>Jonathan D. Chang*</strong>,
							Nishanth Kumar, 
							Sean Hastings,
                            Aaron Gokaslan,
                            Diego Romeres,
                            Devesh Jha,
                            Daniel Nikovski,
                            George Konidaris,
                            Stefanie Tellex
              <br>
              <a href="https://arxiv.org/abs/1910.10628">arXiv</a>
              <p></p>
              <p>We introduce an end-to-end method for targetable visuomotor skills as a goal-parameterized neual network policy, resulting in successfully learning a mapping from target pixel coordinates to a robot policy.</p>
            </td>
          </tr> 

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="100%" valign="center">
                <p>Reviewer: NeurIPS 2021, ICML 2022, ICLR 2022, NeurIPS 2022, NeurIPS 2023, ICML 2023, ICLR 2024, NeurIPS 2024, ICML 2024</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
